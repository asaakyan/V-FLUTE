{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a.saakyan/tmp/ENTER/envs/llava3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm   \n",
    "import random\n",
    "import os\n",
    "import json\n",
    "# shuffle list\n",
    "random.seed(42)\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Does the image entail or contradict the claim REPLACE_CLAIM? Explain your reasoning and provide a label between Entails or Contradicts.\",\n",
    "    \"Is the image consistent with the statement REPLACE_CLAIM? Justify your answer and classify it as either Entails or Contradicts.\",\n",
    "    \"Does the picture support or refute the assertion REPLACE_CLAIM? Offer your rationale and select a label: Entails or Contradicts.\",\n",
    "    \"Can the image be seen as validating or opposing the claim REPLACE_CLAIM? Explain your thought process and assign a label of Entails or Contradicts.\",\n",
    "    \"Is there agreement or disagreement between the image and the claim REPLACE_CLAIM? Provide your analysis and choose between Entails or Contradicts.\",\n",
    "    \"Does this image confirm or deny the claim REPLACE_CLAIM? Discuss your reasoning and determine a label: Entails or Contradicts.\",\n",
    "    \"Is the image in harmony with or in conflict with the statement REPLACE_CLAIM? Explain your justification and label it as Entails or Contradicts.\",\n",
    "    \"Does the image corroborate or dispute the claim REPLACE_CLAIM? Outline your reasoning and categorize it under Entails or Contradicts.\",\n",
    "    \"Is the depiction aligned with or against the claim REPLACE_CLAIM? Share your evaluation and identify it as either Entails or Contradicts.\",\n",
    "    \"Does the visual evidence support or counter the claim REPLACE_CLAIM? Provide your explanation and assign it a label of Entails or Contradicts.\",\n",
    "    \"Is the content of the image endorsing or challenging the claim REPLACE_CLAIM? Justify your position and label it as Entails or Contradicts.\",\n",
    "    \"Does the illustration affirm or negate the claim REPLACE_CLAIM? Articulate your reasoning and apply a label: Entails or Contradicts.\",\n",
    "    \"Is the portrayal in the image consistent with or contradictory to the claim REPLACE_CLAIM? Offer your insights and select between Entails or Contradicts.\",\n",
    "    \"Does the image agree with or dispute the claim REPLACE_CLAIM? Explain your analysis and mark it as Entails or Contradicts.\",\n",
    "    \"Is the image's message supporting or opposing the claim REPLACE_CLAIM? Discuss your rationale and determine the appropriate label: Entails or Contradicts.\",\n",
    "    \"Does the illustration affirm or contest the claim REPLACE_CLAIM? Provide your argument and choose a label: Entails or Contradicts.\",\n",
    "    \"Is the visual portrayal compatible with or adverse to the claim REPLACE_CLAIM? Justify your viewpoint and label it as Entails or Contradicts.\",\n",
    "    \"Does the image's depiction validate or refute the claim REPLACE_CLAIM? Explain your point of view and select a label: Entails or Contradicts.\",\n",
    "    \"Is the visual content in agreement or disagreement with the claim REPLACE_CLAIM? Offer your explanation and categorize it under Entails or Contradicts.\",\n",
    "    \"Does the image's narrative confirm or disprove the claim REPLACE_CLAIM? Discuss your reasoning and identify it as either Entails or Contradicts.\",\n",
    "    \"Is the image's representation supportive of or contradictory to the claim REPLACE_CLAIM? Articulate your analysis and assign the label: Entails or Contradicts.\"\n",
    "]\n",
    "prompts = [p.replace('Entails or Contradicts', 'entailment or contradiction') for p in prompts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert v-flute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = load_dataset(\"ColumbiaNLP/V-FLUTE\", cache_dir=\"./huggingface_cache\", split=\"train\")\n",
    "data_valid = load_dataset(\"ColumbiaNLP/V-FLUTE\", cache_dir=\"./huggingface_cache\", split=\"validation\")\n",
    "data_test = load_dataset(\"ColumbiaNLP/V-FLUTE\", cache_dir=\"./huggingface_cache\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "for split, data_portion in zip([\"train\", \"valid\", \"test\"], \n",
    "                                [data_train, data_valid, data_test]):\n",
    "# for split, data_portion in zip([ \"test\"], \n",
    "#                                 [data_test.select(range(10)) ]):\n",
    "    ft_data = []\n",
    "    for i, row in tqdm(enumerate(data_portion), total=len(data_portion)):\n",
    "\n",
    "        # save image from row[image] to data_dir\n",
    "        # save_dir_path = f\"/mnt/swordfish-pool2/asaakyan/visEntail/data/VFLUTE-v2/{row['source_dataset']}/{split}\"\n",
    "        save_dir_path = \"ENTER YOUR PATH\"\n",
    "        if not os.path.exists(save_dir_path): os.makedirs(save_dir_path)\n",
    "        img_format = \"jpg\" if row['image'].format == \"JPEG\" else \"png\"\n",
    "        # im_path = f\"{save_dir_path}/{i}.{img_format}\"\n",
    "        im_path = f\"{row['source_dataset']}/{split}/{i}.{img_format}\"\n",
    "        row['image'].save(f\"{save_dir_path}/{i}.{img_format}\")\n",
    "        img_id = f\"{row['source_dataset']}-{split}-{i}\"\n",
    "\n",
    "        claim = row['claim'].strip()\n",
    "        expl = row['explanation'].strip()\n",
    "        label = row['label']\n",
    "\n",
    "        # USE ALL PROMPTS \n",
    "        sampled_prompt = random.choice(prompts)\n",
    "        sampled_prompt_repl = sampled_prompt.replace(\"REPLACE_CLAIM\", '\"' + f\"{claim}\" + '\"').strip()\n",
    "\n",
    "        transformed = {\n",
    "            \"id\": img_id,  \n",
    "            \"source_dataset\": row['source_dataset'],\n",
    "            \"phenomenon\": row['phenomenon'],\n",
    "            \"claim\": claim,\n",
    "            \"label\": label,\n",
    "            \"explanation\": expl,\n",
    "            \"prompt\": sampled_prompt,\n",
    "            \"image\": im_path,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"<image>\\n{sampled_prompt_repl}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": f\"{expl}\\nLABEL: {label}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ft_data.append([img_id, \n",
    "                        row['source_dataset'], \n",
    "                        row['phenomenon'],\n",
    "                        im_path,\n",
    "                        claim, \n",
    "                        label,\n",
    "                        expl, \n",
    "                        sampled_prompt,\n",
    "                        transformed])\n",
    "\n",
    "    df = pd.DataFrame(ft_data, columns=[\"id\", \"source_dataset\", \"phenomenon\", \"path\",\n",
    "                                        \"claim\", \"label\", \"explanation\",\n",
    "                                        \"prompt\", \"transformed\"])\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    df.to_csv(f\"../data/flute-v-llava-clean/vflute-v2-{split}.csv\", index=False)\n",
    "    print(df.shape)\n",
    "    print(df['source_dataset'].value_counts())  \n",
    "    print(df['source_dataset'].value_counts()/df.shape[0]*100)  \n",
    "    with open(f'../data/flute-v-llava-clean/vflute-v2-{split}.json', 'w') as f:\n",
    "        json.dump(df['transformed'].to_list(), f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4578/4578 [01:38<00:00, 46.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4578, 9)\n",
      "source_dataset\n",
      "memecap       1566\n",
      "irfl          1082\n",
      "muse           830\n",
      "vismet         649\n",
      "nycartoons     451\n",
      "Name: count, dtype: int64\n",
      "source_dataset\n",
      "memecap       34.207077\n",
      "irfl          23.634775\n",
      "muse          18.130188\n",
      "vismet        14.176496\n",
      "nycartoons     9.851464\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 726/726 [00:14<00:00, 51.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(726, 9)\n",
      "source_dataset\n",
      "irfl          217\n",
      "memecap       196\n",
      "vismet        107\n",
      "muse          106\n",
      "nycartoons    100\n",
      "Name: count, dtype: int64\n",
      "source_dataset\n",
      "irfl          29.889807\n",
      "memecap       26.997245\n",
      "vismet        14.738292\n",
      "muse          14.600551\n",
      "nycartoons    13.774105\n",
      "Name: count, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 723/723 [00:14<00:00, 49.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(723, 9)\n",
      "source_dataset\n",
      "irfl          220\n",
      "memecap       196\n",
      "muse          106\n",
      "vismet        101\n",
      "nycartoons    100\n",
      "Name: count, dtype: int64\n",
      "source_dataset\n",
      "irfl          30.428769\n",
      "memecap       27.109267\n",
      "muse          14.661134\n",
      "vismet        13.969571\n",
      "nycartoons    13.831259\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "for split, data_portion in zip([\"train\", \"valid\", \"test\"], \n",
    "                                [data_train, data_valid, data_test]):\n",
    "# for split, data_portion in zip([ \"test\"], \n",
    "#                                 [data_test.select(range(10)) ]):\n",
    "    ft_data = []\n",
    "    for i, row in tqdm(enumerate(data_portion), total=len(data_portion)):\n",
    "\n",
    "        # save image from row[image] to data_dir\n",
    "        # save_dir_path = f\"/mnt/swordfish-pool2/asaakyan/visEntail/data/VFLUTE-v2/{row['source_dataset']}/{split}\"\n",
    "        # if not os.path.exists(save_dir_path): os.makedirs(save_dir_path)\n",
    "        # img_format = \"jpg\" if row['image'].format == \"JPEG\" else \"png\"\n",
    "        # im_path = f\"{save_dir_path}/{i}.{img_format}\"\n",
    "        im_path = f\"white.png\"\n",
    "        # row['image'].save(f\"{save_dir_path}/{i}.{img_format}\")\n",
    "        img_id = f\"{row['source_dataset']}-{split}-{i}\"\n",
    "\n",
    "        claim = row['claim'].strip()\n",
    "        expl = row['explanation'].strip()\n",
    "        label = row['label']\n",
    "\n",
    "        # USE ALL PROMPTS \n",
    "        sampled_prompt = random.choice(prompts)\n",
    "        sampled_prompt_repl = sampled_prompt.replace(\"REPLACE_CLAIM\", '\"' + f\"{claim}\" + '\"').strip()\n",
    "\n",
    "        transformed = {\n",
    "            \"id\": img_id,  \n",
    "            \"source_dataset\": row['source_dataset'],\n",
    "            \"phenomenon\": row['phenomenon'],\n",
    "            \"claim\": claim,\n",
    "            \"label\": label,\n",
    "            \"explanation\": expl,\n",
    "            \"prompt\": sampled_prompt,\n",
    "            \"image\": im_path,\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"<image>\\n{sampled_prompt_repl}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": f\"{expl}\\nLABEL: {label}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ft_data.append([img_id, \n",
    "                        row['source_dataset'], \n",
    "                        row['phenomenon'],\n",
    "                        im_path,\n",
    "                        claim, \n",
    "                        label,\n",
    "                        expl, \n",
    "                        sampled_prompt,\n",
    "                        transformed])\n",
    "\n",
    "    df = pd.DataFrame(ft_data, columns=[\"id\", \"source_dataset\", \"phenomenon\", \"path\",\n",
    "                                        \"claim\", \"label\", \"explanation\",\n",
    "                                        \"prompt\", \"transformed\"])\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    df.to_csv(f\"../data/flute-v-llava-clean/vflute-v2-noimage-{split}.csv\", index=False)\n",
    "    print(df.shape)\n",
    "    print(df['source_dataset'].value_counts())  \n",
    "    print(df['source_dataset'].value_counts()/df.shape[0]*100)  \n",
    "    with open(f'../data/flute-v-llava-clean/vflute-v2-noimage-{split}.json', 'w') as f:\n",
    "        json.dump(df['transformed'].to_list(), f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10897, 6) (275815, 6) (10939, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275815it [00:15, 17295.63it/s]\n",
      "10897it [00:00, 18311.54it/s]\n",
      "10939it [00:00, 19143.77it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../e-ViL/data\"\n",
    "train_df = pd.read_csv(f\"{data_dir}/esnlive_train.csv\")\n",
    "train_df = train_df[train_df['gold_label'] != 'neutral']\n",
    "valid_df = pd.read_csv(f\"{data_dir}/esnlive_dev.csv\")\n",
    "valid_df = valid_df[valid_df['gold_label'] != 'neutral']\n",
    "test_df = pd.read_csv(f\"{data_dir}/esnlive_test.csv\")\n",
    "test_df = test_df[test_df['gold_label'] != 'neutral']\n",
    "print(valid_df.shape, train_df.shape, test_df.shape)\n",
    "\n",
    "random.seed(42)\n",
    "for split, dataset in zip([\"train\", \"valid\", \"test\"],\n",
    "                                [train_df, valid_df, test_df]):\n",
    "\n",
    "    ft_data = []\n",
    "    for i, row in tqdm(dataset.iterrows()):\n",
    "        #using ALL prompts\n",
    "        sampled_prompt = random.choice(prompts)\n",
    "        sampled_prompt = sampled_prompt.replace(\"REPLACE_CLAIM\", '\"' + f\"{row['hypothesis']}\" + '\"')\n",
    "        transformed = {\n",
    "            \"id\": f\"evil-{split}-{row['Flickr30kID']}\", \n",
    "            \"image\": f\"evil/flickr30k_images/flickr30k_images/{row['Flickr30kID']}\",\n",
    "            \"conversations\": [\n",
    "                {x\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": f\"<image>\\n{sampled_prompt}\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": f\"{row['explanation']}\\nLABEL: {row['gold_label']}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ft_data.append(transformed)\n",
    "    with open(f'../data/evil-llava-clean/{split}.json', 'w') as f:\n",
    "        json.dump(ft_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eViL+vflute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/flute-v-llava-clean/vflute-v2-train.json', 'r') as f:\n",
    "    train_json_vflute= json.load(f)\n",
    "for row in train_json_vflute:\n",
    "    row['image'] = \"VFLUTE-v2\" + \"/\" + row['image']\n",
    "train_json_vflute[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/flute-v-llava-clean/vflute-v2-valid.json', 'r') as f:\n",
    "    valid_json_vflute= json.load(f)\n",
    "for row in valid_json_vflute:\n",
    "    row['image'] = \"VFLUTE-v2\" + \"/\" + row['image']\n",
    "with open('../data/flute-v-llava-clean/evil_vflute_valid.json', 'w') as f:\n",
    "    json.dump(valid_json_vflute, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'evil-train-4564320256.jpg',\n",
       " 'image': 'evil/flickr30k_images/flickr30k_images/4564320256.jpg',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nIs the image\\'s representation supportive of or contradictory to the claim \"Two old men robbing a convenience store.\"? Articulate your analysis and assign the label: entailment or contradiction.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'A lady and her granddaughter cannot also be two men\\nLABEL: contradiction'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/evil-llava-clean/train.json', 'r') as f:\n",
    "    train_json_evil = json.load(f)\n",
    "# for row in train_json_evil:\n",
    "#     row['image'] = \"evil/flickr30k_images/flickr30k_images\" + \"/\" + row['image']\n",
    "train_json_evil[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275815 4578 280393\n"
     ]
    }
   ],
   "source": [
    "train_json_evil_flute = train_json_evil + train_json_vflute \n",
    "print(len(train_json_evil), len(train_json_vflute), len(train_json_evil_flute))\n",
    "with open('../data/flute-v-llava-clean/evil_vflute_train.json', 'w') as f:\n",
    "    json.dump(train_json_evil_flute, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "creativelm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
